{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Reduce decimal points to 2\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "import statistics \n",
    "from statistics import mode \n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "#import psycopg2\n",
    "import getopt\n",
    "import sys\n",
    "import math\n",
    "import decimal\n",
    "import copy\n",
    "import csv\n",
    "import progressbar\n",
    "import time\n",
    "#from toolkit import Date\n",
    "from datetime import timedelta\n",
    "import networkx as nx\n",
    "#from network_viz import toGraph\n",
    "\n",
    "import json \n",
    "import itertools\n",
    "\n",
    "#from network_viz import output_Network_Viz\n",
    "#from network_viz import get_Network_Viz_Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.strptime('2020-01-25', '%Y-%m-%d')\n",
    "#end_date = datetime.strptime('2020-04-04', '%Y-%m-%d')\n",
    "end_date = datetime.strptime(datetime.today().strftime(\"%Y-%m-%d\"), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read userData csv and save as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "userData_raw = pd.read_csv('/Users/rebeccamqamelo/Desktop/Capstone/users_all_pub_Nov2020.csv', header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correct datetime format\n",
    "userData_raw['start'] = pd.to_datetime(userData_raw['start'], format='%Y-%m-%d').dt.floor('d')\n",
    "userData_raw.sort_values(by='start', inplace=True)\n",
    "\n",
    "# Fix messy genders\n",
    "nans = userData_raw.index[userData_raw['gender'].isnull()].tolist()\n",
    "userData_raw.loc[(userData_raw.index.isin(nans)) & (userData_raw.held_roles != 'ADMIN'), 'gender'] = 'unknown gender'\n",
    "userData_raw.loc[(userData_raw.index.isin(nans)) & (userData_raw.held_roles == 'ADMIN'), 'gender'] = 'system'\n",
    "userData_raw.gender = userData_raw.gender.str.lower()\n",
    "userData_raw.gender = userData_raw.gender.replace('male ', 'male')\n",
    "\n",
    "userData_raw.to_csv('/Users/rebeccamqamelo/Desktop/Capstone/userData_raw_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.DictReader(open('/Users/rebeccamqamelo/Desktop/Capstone/userData_raw_sorted.csv'))\n",
    "\n",
    "userData_dict = {}\n",
    "for row in reader:\n",
    "    key = row.pop('xDAI_blockchain_address')\n",
    "    if key in userData_dict:\n",
    "        # implement your duplicate row handling here\n",
    "        pass\n",
    "    userData_dict[key] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read txnData csv and save as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "txnData_raw = pd.read_csv('/Users/rebeccamqamelo/Desktop/Capstone/tx_all_pub_Nov2020.csv', header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correct datetime format\n",
    "txnData_raw['timeset'] = pd.to_datetime(txnData_raw['timeset'], format='%Y-%m-%d').dt.floor('d')\n",
    "txnData_raw.sort_values(by='timeset', inplace=True)\n",
    "\n",
    "# Fix messy genders\n",
    "s_nans = txnData_raw.index[txnData_raw['s_gender'].isnull()].tolist()\n",
    "txnData_raw.loc[(txnData_raw.index.isin(nans)), 's_gender'] = 'unknown gender'\n",
    "txnData_raw.loc[(txnData_raw.s_business_type == 'System'), 's_gender'] = 'system'\n",
    "txnData_raw.s_gender = txnData_raw.s_gender.str.lower()\n",
    "txnData_raw.s_gender = txnData_raw.s_gender.replace('male ', 'male')\n",
    "\n",
    "t_nans = txnData_raw.index[txnData_raw['t_gender'].isnull()].tolist()\n",
    "txnData_raw.loc[(txnData_raw.index.isin(nans)), 't_gender'] = 'unknown gender'\n",
    "txnData_raw.loc[(txnData_raw.t_business_type == 'System'), 't_gender'] = 'system'\n",
    "txnData_raw.t_gender = txnData_raw.t_gender.str.lower()\n",
    "txnData_raw.t_gender = txnData_raw.t_gender.replace('male ', 'male')\n",
    "\n",
    "txnData_raw.to_csv('/Users/rebeccamqamelo/Desktop/Capstone/txnData_raw_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approx. 8 min to run!\n",
    "reader = csv.DictReader(open('/Users/rebeccamqamelo/Desktop/Capstone/txnData_raw_sorted.csv'))\n",
    "\n",
    "txnData_dict = {}\n",
    "for row in reader:\n",
    "    date = row['timeset']\n",
    "    sender_key = row['source']\n",
    "    receiver_key = row['target']\n",
    "    if sender_key in txnData_dict: # user's dict has already been created\n",
    "        txnData_dict[sender_key].append({date: row})\n",
    "    else:\n",
    "        txnData_dict[sender_key] = [] # empty array to contain dict of each txn for that user\n",
    "        txnData_dict[sender_key].append({date: row}) \n",
    "        \n",
    "    if receiver_key in txnData_dict: # other user's dict has already been created\n",
    "        txnData_dict[receiver_key].append({date: row})\n",
    "    else:\n",
    "        txnData_dict[receiver_key] = [] # empty array to contain dict of each txn for the other user\n",
    "        txnData_dict[receiver_key].append({date: row}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update userData by time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "userData = userData_dict\n",
    "txnData = txnData_dict\n",
    "#userData = dict(itertools.islice(userData_dict.items(), 30))  \n",
    "#txnData = dict(itertools.islice(txnData_dict.items(), 30))  \n",
    "\n",
    "headersUserPub = ['id', 'start', 'label', 'gender', 'location', 'lat','lon', \n",
    "              'held_roles', 'business_type', 'bal', 'xDAI_blockchain_address', \n",
    "              'ovol_in', 'ovol_out', 'otxns_in', 'otxns_out', 'ounique_in', 'ounique_out', \n",
    "              'svol_in', 'svol_out', 'stxns_in', 'stxns_out', 'sunique_in', 'sunique_out',\n",
    "              'sunique_out_group', 'sunique_in_at', 'sunique_out_at', 'sunique_out_at_group',\n",
    "              'days_enrolled', 'days_active', 'trades_in_daily', 'trades_out_daily', \n",
    "              'trade_partners_in_daily', 'trade_partners_out_daily', 'gender_ratio_in', 'gender_ratio_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_userData(userData, txnData, start_date, end_date, headersUserPub):\n",
    "\n",
    "    for user in userData.keys():\n",
    "        volume_in = 0\n",
    "        volume_out = 0\n",
    "        txns_in = 0 # all trades\n",
    "        txns_out = 0\n",
    "        unique_txns_in = 0 # unique trades\n",
    "        unique_txns_out = 0\n",
    "\n",
    "        svolume_in = 0\n",
    "        svolume_out = 0\n",
    "        stxns_out = 0\n",
    "        stxns_in = 0\n",
    "        sunique_txns_out = 0\n",
    "        sunique_txns_out_group = 0\n",
    "        sunique_txns_in = 0\n",
    "\n",
    "        sunique_txns_out_atleast = 0\n",
    "        stotal_unique_txns_out_atleast = 0\n",
    "        stotal_unique_txns_out_atleast_group = 0\n",
    "        #stotal_unique_txns_out_atleast_group += 1\n",
    "        sunique_txns_out_atleast_group = 0\n",
    "        sunique_txns_in_atleast = 0\n",
    "        \n",
    "        females_sold_to = 0\n",
    "        males_sold_to = 0\n",
    "        females_bought_from = 0\n",
    "        males_bought_from = 0\n",
    "\n",
    "        min_size = 5 # min Sarafu trade amount (?)\n",
    "\n",
    "        sseenRecUsers = []\n",
    "        seenRecUsers = []\n",
    "\n",
    "        sseenSentUsers = []\n",
    "        seenSentUsers = []\n",
    "\n",
    "        if(user in txnData.keys()): # if user has participated in a trade in the txnData dict\n",
    "            txns = txnData[user] # list containing dicts for each txn\n",
    "            #for trans in txnData[user]: # for each txn in the user's txn array\n",
    "            for txn in txns: # parses each txn dict, where the key is the date\n",
    "                date_str = list(txn.keys())[0]\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                if start_date <= date <= end_date: # checks that the date is within the defined range\n",
    "                    trans = txn[date_str]\n",
    "                    #for trans in txn:\n",
    "                    if trans['source'] == user:\n",
    "                        if trans['transfer_subtype'] != 'STANDARD':\n",
    "                            volume_out+=float(trans['weight'])\n",
    "                            txns_out+=1\n",
    "                            if trans['target'] not in seenRecUsers:\n",
    "                                seenRecUsers.append(trans['target'])\n",
    "                                unique_txns_out+=1\n",
    "\n",
    "                        else: # standard txn with other users\n",
    "                            svolume_out+=float(trans['weight'])\n",
    "                            stxns_out+=1\n",
    "                            \n",
    "                            if userData[trans['target']]['gender'] == 'female':\n",
    "                                females_sold_to+=1\n",
    "                            elif userData[trans['target']]['gender'] == 'male':\n",
    "                                males_sold_to+=1\n",
    "                            \n",
    "                            #if (user == 4082):\n",
    "                            #    print(stxns_out)\n",
    "                            if trans['target'] not in sseenRecUsers: # txn is standard and unqiue\n",
    "                                sseenRecUsers.append(trans['target'])\n",
    "                                sunique_txns_out+=1\n",
    "                                if userData[user]['held_roles'] == \"GROUP_ACCOUNT\":\n",
    "                                    sunique_txns_out_group += 1\n",
    "                                if(float(trans['weight'])>=min_size):\n",
    "                                    sunique_txns_out_atleast += 1\n",
    "                                    if sunique_txns_out_atleast > 1:\n",
    "                                        stotal_unique_txns_out_atleast += 1\n",
    "                                    if userData[user]['held_roles'] == \"GROUP_ACCOUNT\":\n",
    "                                        sunique_txns_out_atleast_group += 1\n",
    "                                        if sunique_txns_out_atleast_group > 1:\n",
    "                                            stotal_unique_txns_out_atleast_group += 1\n",
    "\n",
    "\n",
    "                    else: # this txn marks where the user was a recipient\n",
    "                        if trans['transfer_subtype'] != 'STANDARD':\n",
    "                            #if user == 13488:\n",
    "                            #    print(\"<><><><> \",trans['_transfer_amount_wei'], trans['created'])\n",
    "                            volume_in+=float(trans['weight'])\n",
    "                            txns_in+=1\n",
    "                            \n",
    "                            if userData[trans['source']]['gender'] == 'female':\n",
    "                                females_bought_from+=1\n",
    "                            elif userData[trans['source']]['gender'] == 'male':\n",
    "                                males_bought_from+=1\n",
    "                            \n",
    "                            if trans['source'] not in seenSentUsers:\n",
    "                                seenSentUsers.append(trans['source'])\n",
    "                                unique_txns_in+=1\n",
    "                            \n",
    "                        else:\n",
    "                            svolume_in+=float(trans['weight'])\n",
    "                            stxns_in+=1\n",
    "                            if trans['source'] not in sseenSentUsers:\n",
    "                                sseenSentUsers.append(trans['source'])\n",
    "                                sunique_txns_in+=1\n",
    "                                if(float(trans['weight'])>=min_size):\n",
    "                                    sunique_txns_in_atleast+=1\n",
    "\n",
    "                                    \n",
    "            # kinda bad practice: using individual txn keys, which are dates, to parse first and last txn\n",
    "            # NB first txn is NOT 0-indexed because this would be their disbursement i.e registration txn\n",
    "            # â€“ this doesn't count as \"active trading\"\n",
    "            \n",
    "            if len(txnData[user]) == 1: \n",
    "                first_trade_date = datetime.strptime(list(txnData[user][0].keys())[0], '%Y-%m-%d') \n",
    "            elif len(txnData[user]) > 1:\n",
    "                first_trade_date = datetime.strptime(list(txnData[user][1].keys())[0], '%Y-%m-%d') \n",
    "            last_trade_date = datetime.strptime(list(txnData[user][-1].keys())[0], '%Y-%m-%d')\n",
    "            #start = datetime.strptime(userData[user]['start'].to_string(index=False), '%Y-%m-%d')\n",
    "            start = datetime.strptime(userData[user]['start'], '%Y-%m-%d')\n",
    "            end = end_date\n",
    "\n",
    "            days_enrolled = (end - start).days\n",
    "            days_active = abs(last_trade_date - first_trade_date).days\n",
    "            \n",
    "            # Avoid division by zero:\n",
    "            if days_enrolled == 0:\n",
    "                days_enrolled = 1\n",
    "            if days_active == 0:\n",
    "                days_active = 1\n",
    "\n",
    "            trades_in_daily = stxns_in/days_active\n",
    "            trades_out_daily = stxns_out/days_active\n",
    "            trade_partners_in_daily = sunique_txns_in/days_active\n",
    "            trade_partners_out_daily = sunique_txns_out/days_active\n",
    "            \n",
    "            # Avoid division by zero:\n",
    "            if males_sold_to == 0:\n",
    "                males_sold_to = 1\n",
    "            if males_bought_from == 0:\n",
    "                males_bought_from = 1\n",
    "                \n",
    "            gender_ratio_out = females_sold_to/males_sold_to\n",
    "            gender_ratio_in = females_bought_from/males_bought_from\n",
    "            #total_unique_out_atleast += stotal_unique_txns_out_atleast\n",
    "        \n",
    "        txData = {'ovol_in':volume_in, 'ovol_out':volume_out, 'otxns_in':txns_in, 'otxns_out':txns_out,\n",
    "                  'ounique_in':unique_txns_in, 'ounique_out':unique_txns_out, 'svol_in':svolume_in, \n",
    "                  'svol_out':svolume_out, 'stxns_in':stxns_in, 'stxns_out':stxns_out, \n",
    "                  'sunique_in':sunique_txns_in, 'sunique_out':sunique_txns_out,\n",
    "                  'sunique_out_group':sunique_txns_out_group, \n",
    "                  'sunique_in_at':sunique_txns_in_atleast,\n",
    "                  'sunique_out_at':sunique_txns_out_atleast, \n",
    "                  'sunique_out_at_group':sunique_txns_out_atleast_group,\n",
    "                  'days_enrolled': days_enrolled, \n",
    "                  'days_active': days_active, \n",
    "                  'trades_in_daily': trades_in_daily, \n",
    "                  'trades_out_daily': trades_out_daily, \n",
    "                  'trade_partners_in_daily': trade_partners_in_daily, \n",
    "                  'trade_partners_out_daily': trade_partners_out_daily,\n",
    "                  'gender_ratio_in': gender_ratio_in,\n",
    "                  'gender_ratio_out': gender_ratio_out}\n",
    "\n",
    "        uDict = userData[user]\n",
    "        uDict.update(txData)\n",
    "        userData[user] = uDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approx. _ min t run!\n",
    "create_userData(userData, txnData, start_date, end_date, headersUserPub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_and_transaction_data_github_csv(txnData, userData, unique_txnData, start_date, end_date, days_ago_str, headersUserPub, private=False):\n",
    "\n",
    "    headersTxPub = ['id', 'timeset', 'transfer_subtype', 'transfer_use','source', 's_comm_tkn', 's_gender', 's_location_path', 's_location_lat','s_location_lon',\n",
    "                     's_business_type', 'target', 't_comm_tkn', 't_gender',\n",
    "                    't_location_path', 't_location_lat', 't_location_lon',\n",
    "                     't_business_type', 'tx_token', 'weight', 'type','token_name', 'token_address']\n",
    "\n",
    "    headersUser = headersUserPub\n",
    "    headersTx = headersTxPub\n",
    "    \n",
    "    #filenameTx = '/Users/rebeccamqamelo/Desktop/Capstone/tx_all_pub_'+start_date.strftime(\"%Y-%m-%d\")+\"-\"+end_date.strftime(\"%Y-%m-%d\")+\"-\"+days_ago_str+'.csv'\n",
    "    #filenameUser = '/Users/rebeccamqamelo/Desktop/Capstone/users_all_pub_'+start_date.strftime(\"%Y-%m-%d\")+\"-\"+end_date.strftime(\"%Y-%m-%d\")+\"-\"+days_ago_str+'.csv'\n",
    "    filenameTx = '/Users/rebeccamqamelo/Desktop/Capstone/tx_all_pub_20200125-20201109-all_time.csv'\n",
    "    filenameUser = '/Users/rebeccamqamelo/Desktop/Capstone/users_all_pub_20200125-20201109-all_time.csv'\n",
    "    \n",
    "    print(\"saving all transactions to: \", filenameTx)\n",
    "    print(\"saving all users to: \", filenameUser)\n",
    "\n",
    "    #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestr = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    token_transactions = txnData\n",
    "\n",
    "    indexR = 0\n",
    "\n",
    "    seen_users = []\n",
    "    exclude_list = []\n",
    "\n",
    "    if True:\n",
    "        numberTx = 1\n",
    "        numberUsers = 0\n",
    "\n",
    "        with open(filenameTx, 'w', newline='') as csvfileTx, open(filenameUser, 'w', newline='') as csvfileUser:\n",
    "            #spamwriterTx = csv.writer(csvfileTx)\n",
    "            #spamwriterTx.writerow(headersTx)\n",
    "\n",
    "            spamwriterUser = csv.writer(csvfileUser)\n",
    "            spamwriterUser.writerow(headersUser)\n",
    "\n",
    "            for user_id, user_info in userData.items():\n",
    "\n",
    "                user_data1 = {'start': user_info['start']}\n",
    "                user_data1['id'] = user_info['id']\n",
    "                user_data1['label'] = user_info['id']\n",
    "                user_data1['xDAI_blockchain_address'] = user_id\n",
    "                #user_data1['xDAI_blockchain_address'] = user_info['xDAI_blockchain_address']\n",
    "\n",
    "                #user_data1['comm_tkn'] = user_info['comm_tkn']\n",
    "\n",
    "                user_data1['bal'] = user_info['bal']\n",
    "                user_data1['gender'] = user_info.get('gender')\n",
    "                user_data1['location'] = user_info.get('location')\n",
    "                user_data1['lat'] = user_info.get('lat')\n",
    "                user_data1['lon'] = user_info.get('lon')\n",
    "                user_data1['held_roles'] = user_info['held_roles']\n",
    "                user_data1['business_type'] = user_info['business_type']\n",
    "                user_data1['ovol_in'] = user_info['ovol_in']\n",
    "                user_data1['ovol_out'] = user_info['ovol_out']\n",
    "                user_data1['otxns_in'] = user_info['otxns_in']\n",
    "                user_data1['otxns_out'] = user_info['otxns_out']\n",
    "                user_data1['ounique_in'] = user_info['ounique_in']\n",
    "                user_data1['ounique_out'] = user_info['ounique_out']\n",
    "                user_data1['svol_in'] = user_info['svol_in']\n",
    "                user_data1['svol_out'] = user_info['svol_out']\n",
    "                user_data1['stxns_in'] = user_info['stxns_in']\n",
    "                user_data1['stxns_out'] = user_info['stxns_out']\n",
    "                user_data1['sunique_in'] = user_info['sunique_in']\n",
    "                user_data1['sunique_out'] = user_info['sunique_out']\n",
    "                user_data1['sunique_out_group'] = user_info['sunique_out_group']\n",
    "                user_data1['sunique_in_at'] = user_info['sunique_in_at']\n",
    "                user_data1['sunique_out_at'] = user_info['sunique_out_at']\n",
    "                user_data1['sunique_out_at_group'] = user_info['sunique_out_at_group']\n",
    "                user_data1['days_enrolled'] = user_info['days_enrolled']\n",
    "                user_data1['days_active'] = user_info['days_active']\n",
    "                user_data1['trades_in_daily'] = user_info['trades_in_daily']\n",
    "                user_data1['trades_out_daily'] = user_info['trades_out_daily']\n",
    "                user_data1['trade_partners_in_daily'] = user_info['trade_partners_in_daily']\n",
    "                user_data1['trade_partners_out_daily'] = user_info['trade_partners_out_daily']\n",
    "                user_data1['gender_ratio_in'] = user_info['gender_ratio_in']\n",
    "                user_data1['gender_ratio_out'] = user_info['gender_ratio_out']\n",
    "                \n",
    "                #user_data1['confidence'] = user_info['confidence']\n",
    "\n",
    "                numberUsers+=1\n",
    "\n",
    "                spamwriterUser.writerow([str(user_data1[k]) for k in headersUser])\n",
    "\n",
    "            c_idx = 0\n",
    "            chunks = 10000\n",
    "            tx_hash = []\n",
    "\n",
    "            \"\"\"\n",
    "            #for tnsfer_acct__id, transactions in txnData.items():\n",
    "            if len(unique_txnData) > 0:\n",
    "                for key, t in unique_txnData.items():\n",
    "                #for t in unique_txnData:\n",
    "                    #if t['id'] in tx_hash:  # only looking at unique data: note that without this there will be double counting on transactions\n",
    "                    #    print(\"Error: duplicate transaction found!\", t['id'])\n",
    "                    #    time.sleep(1.5)\n",
    "                    #    continue\n",
    "                    #tx_hash.append(t['id'])\n",
    "\n",
    "                    sender_user_id = t['source']\n",
    "                    print(sender_user_id)\n",
    "                    recipient_user_id = t['target']\n",
    "                    print(recipient_user_id)\n",
    "\n",
    "                    if sender_user_id == None:\n",
    "                        sender_user_id = 1\n",
    "                    #elif sender_user_id == '0xEDA5C9B75Fdb3B9bdAB987A704632280Cf93084F': # Grassroots Economics System\n",
    "                        #sender_user_id = 1\n",
    "\n",
    "                    if recipient_user_id == None:\n",
    "                        recipient_user_id = 1\n",
    "\n",
    "                    row_data = {'timeset': t['timeset']}\n",
    "\n",
    "                    row_data['weight'] = t['weight']\n",
    "                    row_data['type'] = 'directed'\n",
    "                    row_data['transfer_subtype'] = t['transfer_subtype']\n",
    "                    row_data['id'] = t['id']\n",
    "                    #row_data['label'] = t['label']\n",
    "                    #if private:\n",
    "                        #row_data['tx_hash'] = t['tx_hash']\n",
    "                    row_data['token_name'] = t['token_name']\n",
    "                    row_data['authorising_user_id'] = t.get('authorising_user_id') #t['authorising_user_id']\n",
    "                    row_data['token_address'] = t['token_address']\n",
    "                    \n",
    "                    #if t['transfer_use'] != None:\n",
    "                        #row_data['transfer_use'] = t['transfer_use'][0].strip('\"[]')\n",
    "                    #else:\n",
    "                        #row_data['transfer_use'] = ''\n",
    "                    row_data['transfer_use'] = t['transfer_use']\n",
    "\n",
    "                    #row_data['source'] = userData[sender_user_id]['blockchain_address']\n",
    "                    row_data['source'] = userData[sender_user_id]['xDAI_blockchain_address']\n",
    "                    row_data['s_location_path'] = userData[sender_user_id].get('s_location_path')\n",
    "                    row_data['s_location_lat'] = userData[sender_user_id].get('s_location_lat')\n",
    "                    row_data['s_location_lon'] = userData[sender_user_id].get('s_location_lon')\n",
    "                    row_data['t_location_path'] = userData[recipient_user_id].get('t_location_path')\n",
    "                    row_data['t_location_lat'] = userData[recipient_user_id].get('t_location_lat')\n",
    "                    row_data['t_location_lon'] = userData[recipient_user_id].get('t_location_lon')\n",
    "                    row_data['s_gender'] = userData[sender_user_id].get('s_gender')\n",
    "                    row_data['s_comm_tkn'] = userData[sender_user_id]['s_comm_tkn']\n",
    "                    row_data['s_business_type'] = userData[sender_user_id]['s_business_type']\n",
    "\n",
    "                    row_data['target'] = userData[recipient_user_id]['target']\n",
    "                    if True:\n",
    "                        row_data['t_gender'] = userData[recipient_user_id].get('gender')\n",
    "                        row_data['t_comm_tkn'] = userData[recipient_user_id]['t_comm_tkn']\n",
    "                        row_data['t_business_type'] = userData[recipient_user_id]['t_business_type']\n",
    "\n",
    "                    if True: #Admin ONLY row_data['authorising_user_id'] is not None and row_data['authorising_user_id'] != 6: #Admin ONLY\n",
    "                        rowString = []\n",
    "                        for k in headersTx:\n",
    "                            if k in row_data.keys():\n",
    "                                #spamwriterTx.writerow([str(row_data[k]) for k in headersTx])\n",
    "                                rowString.append(str(row_data[k]))\n",
    "                            else:\n",
    "                                rowString.append('')\n",
    "\n",
    "                        spamwriterTx.writerow(rowString)\n",
    "                        if indexR < 3:\n",
    "                            #print(row_data) #debug\n",
    "                            indexR+=1\n",
    "                        if c_idx >= chunks:\n",
    "                            c_idx = 0\n",
    "                            csvfileTx.flush()  # whenever you want\n",
    "                            print(\"chunk: \", numberTx)\n",
    "                        else:\n",
    "                            c_idx += 1\n",
    "\n",
    "                        numberTx += 1\n",
    "\n",
    "        print(\"****saved all transactions to csv\", filenameTx, \" number of tx:\", numberTx, timestr)\n",
    "            \n",
    "            \"\"\"  \n",
    "            \n",
    "        print(\"****saved all users to csv\", filenameUser, \" number of User:\", numberUsers, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving all transactions to:  /Users/rebeccamqamelo/Desktop/Capstone/tx_all_pub_20200125-20201109-all_time.csv\n",
      "saving all users to:  /Users/rebeccamqamelo/Desktop/Capstone/users_all_pub_20200125-20201109-all_time.csv\n",
      "****saved all users to csv /Users/rebeccamqamelo/Desktop/Capstone/users_all_pub_20200125-20201109-all_time.csv  number of User: 40941 2020-11-13\n"
     ]
    }
   ],
   "source": [
    "unique_txnData = txnData.copy()\n",
    "days_ago_str = \"all_time\"\n",
    "generate_user_and_transaction_data_github_csv(txnData, userData, unique_txnData, start_date, end_date, days_ago_str, headersUserPub, private=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
